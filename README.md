# ðŸ“š Local Multi-PDF Conversational RAG System 

* **LangChain** â€“ LLM orchestration framework
* **Ollama** â€“ Local LLM runtime
* **Chroma** â€“ Vector database
* **Llama 3.2** â€“ Language model

---

# ðŸ”¹ 1. Configuration Section

```python
PDF_FOLDER = "./my_notebook" 
DB_DIR = "./chroma_db"
MODEL_NAME = "llama3.2"
```

* `PDF_FOLDER` â†’ Folder containing your PDFs
* `DB_DIR` â†’ Directory where the vector database is stored
* `MODEL_NAME` â†’ LLM used via Ollama

---

# ðŸ”¹ 2. Multi-PDF Loading

## Function: `load_all_pdfs(folder_path)`

### Step 1 â€” Find All PDFs

```python
pdf_files = glob.glob(os.path.join(folder_path, "*.pdf"))
```

Finds all `.pdf` files inside the specified folder.

---

### Step 2 â€” Load Each PDF

```python
loader = PyPDFLoader(pdf)
all_docs.extend(loader.load())
```

* `PyPDFLoader` loads each PDF
* Converts every page into a `Document` object

Each `Document` contains:

* `page_content`
* `metadata`

---

### Step 3 â€” Split Into Chunks

```python
RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
```

Why splitting?

LLMs cannot process very large documents efficiently.
So we:

* Split into chunks of **800 characters**
* Keep **100 character overlap** to preserve context continuity

This improves retrieval quality.

---

# ðŸ”¹ 3. Vector Database Initialization

## Embedding Model

```python
embeddings = OllamaEmbeddings(model="mxbai-embed-large")
```

Embeddings convert text â†’ numerical vectors.

This allows similarity search in vector space.

---

## Case 1 â€” Database Does NOT Exist

```python
Chroma.from_documents(...)
```

Steps:

1. Convert text chunks â†’ embeddings
2. Store embeddings in Chroma
3. Persist database to disk

---

## Case 2 â€” Database Already Exists

```python
Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
```

Loads the existing vector index (much faster).

---

## Retriever

```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
```

When a query is asked:

* Convert query â†’ embedding
* Retrieve **top 5 most similar chunks**

---

# ðŸ”¹ 4. Conversational RAG Logic

This system implements **History-Aware Retrieval**.

---

# ðŸ§  Part A â€” Question Rewriting

### The Problem

If a user asks:

> "What about its limitations?"

This question depends on previous context.

So we rewrite it into a **standalone question**.

---

## Contextualization Prompt

```python
contextualize_q_prompt = ChatPromptTemplate.from_messages(...)
```

Structure:

* System instructions
* Chat history
* Latest user question

---

## History-Aware Retriever Chain

```python
history_aware_retriever = (
    contextualize_q_prompt 
    | llm 
    | StrOutputParser() 
    | retriever
)
```

Pipeline:

```
User Question
      â†“
LLM rewrites question
      â†“
Standalone Question
      â†“
Retriever
      â†“
Relevant Documents
```

---

# ðŸ§  Part B â€” Final Answer Generation

Now we generate the answer using:

* Retrieved context
* Chat history
* Original user question

---

## QA Prompt

```python
qa_prompt = ChatPromptTemplate.from_messages(...)
```

The system message instructs:

* Use only the retrieved context
* If answer not found â†’ say "I don't know"

This helps reduce hallucination.

---

## Formatting Retrieved Documents

```python
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
```

This converts a list of documents into one combined text block.

---

# ðŸ”¥ The Main RAG Chain

```python
rag_chain = (
    RunnablePassthrough.assign(
        context=lambda x: format_docs(history_aware_retriever.invoke(x))
    )
    | qa_prompt
    | llm
    | StrOutputParser()
)
```

---

## Step-by-Step Execution

When:

```python
rag_chain.invoke({
    "input": user_input,
    "chat_history": chat_history
})
```

It performs:

### 1ï¸âƒ£ Retrieve Context

* Rewrites question
* Retrieves top 5 relevant chunks

### 2ï¸âƒ£ Format Context

* Converts documents into text

### 3ï¸âƒ£ Inject Into Prompt

* Adds context to QA prompt

### 4ï¸âƒ£ Send to LLM

* Uses `llama3.2`

### 5ï¸âƒ£ Parse Output

* Extract plain string response

---

# ðŸ”¹ 5. Chat Loop

```python
while True:
```

Steps:

1. Receive user input
2. Call `rag_chain`
3. Print response
4. Update chat history
5. Keep only last 10 messages

---

## Why Limit Chat History?

```python
if len(chat_history) > 10:
    chat_history = chat_history[-10:]
```

Prevents:

* Memory overflow
* Large prompts
* Slow inference

---

# ðŸ§  Overall Architecture

```
User Question
      â†“
Question Rewriter (LLM)
      â†“
Retriever (Chroma)
      â†“
Top 5 Chunks
      â†“
QA Prompt
      â†“
LLM (Llama3.2)
      â†“
Final Answer
```

---

# âœ… System Features

âœ” Multi-PDF ingestion
âœ” Persistent vector database
âœ” History-aware retrieval
âœ” Retrieval-Augmented Generation
âœ” Conversational memory
âœ” Fully local (no external API)

---

# ðŸŽ¯ Summary

This code implements a **production-style local conversational RAG pipeline** that:

* Loads multiple PDFs
* Splits them into chunks
* Embeds and stores them in Chroma
* Retrieves relevant context
* Rewrites dependent questions
* Generates grounded answers using LLM
